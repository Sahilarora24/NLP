{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mBrRIvmhBMoVFy7pjiwAfl9fYCjYvfMu",
      "authorship_tag": "ABX9TyP1RdPS2aJWjSRZRtQDpQ7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahilarora24/NLP/blob/main/Getting_started_with_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zJTHwtITD1WO"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install -q -U google-genai\n",
        "!pip install -U langchain langchain-openai\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "L4x3AJ9xF0rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('/content/drive/MyDrive/Environment Variables/.env')\n",
        "os.environ['OPEN_API_KEY']=os.getenv(\"OPEN_API_KEY\")\n",
        "os.environ['GEMINI_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "5329k4TOF_Kl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "llm.invoke(\"Hello, world!\")"
      ],
      "metadata": {
        "id": "oDNWYPo1LNAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read the API key from Collab Secrets**"
      ],
      "metadata": {
        "id": "CDvH_hhZaL_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "0baYSmyNXTVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d65ca7a-16d2-422c-f02c-977f9ea706e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyDcHElhC57Sgj6NOjXkDlilOweJ8rYHj98'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read the API key from .env file**"
      ],
      "metadata": {
        "id": "rhvqNUXqaWJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv('/content/drive/MyDrive/Environment Variables/.env')\n",
        "\n",
        "gemini_key = os.getenv(\"GOOGLE_API_KEY\").strip()\n",
        "print(\"Gemini API Key:\", repr(gemini_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wphv_Pk1ZHR_",
        "outputId": "bb2f2392-c64f-425f-8654-be94832a0ffe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key: 'AIzaSyDcHElhC57Sgj6NOjXkDlilOweJ8rYHj98'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "2o9-Q5j8ZrvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "87PN8BlBfmbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv('/content/drive/MyDrive/Environment Variables/.env')\n",
        "\n",
        "project = os.getenv(\"LANGSMITH_PROJECT\").strip()\n",
        "print(repr(project))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWTaYy1RhM1O",
        "outputId": "430c903b-2f08-4716-e2e8-6872162adb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Getting Started with Langchain with Google Gemini'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJC-7Po2pFjv",
        "outputId": "34e77c8d-8152-4f0a-d316-537a1e68314a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='models/gemini-2.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f437a0f2d10> default_metadata=() model_kwargs={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LangSmith API Key:\", repr(os.getenv(\"LANGSMITH_API_KEY\")))\n",
        "print(\"LangSmith Project:\", repr(os.getenv(\"LANGSMITH_PROJECT\")))\n",
        "print(\"LangSmith Endpoint:\", repr(os.getenv(\"LANGSMITH_ENDPOINT\")))\n",
        "print(\"LangSmith Tracing:\", repr(os.getenv(\"LANGSMITH_TRACING\")))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vijZ9kVbqNDQ",
        "outputId": "95a8ed60-d9ee-4f4f-b613-f14d7b36e293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith API Key: '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'\n",
            "LangSmith Project: 'Getting Started with Langchain with Google Gemini'\n",
            "LangSmith Endpoint: 'https://api.smith.langchain.com'\n",
            "LangSmith Tracing: 'true'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Force a LangSmith trace using LangChain + Gemini\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Gemini model wrapped with LangChain (this integrates with LangSmith)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Invoke a prompt (this should create a trace in your LangSmith project)\n",
        "response = llm.invoke(\"Hello LangSmith, can you confirm this trace?\")\n",
        "print(\"LLM Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMmyJmSJq_Nv",
        "outputId": "ed90afba-4c6f-4088-83c9-a9e07e13911d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=4f1d2579-cb98-45d9-a06c-3c06aa12e568,id=4f1d2579-cb98-45d9-a06c-3c06aa12e568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response: content=\"Hello!\\n\\nI am an AI assistant, not the LangSmith platform itself. Therefore, I cannot directly access or confirm specific traces within your LangSmith account.\\n\\nIf you'd like to describe the trace or the problem you're encountering, I might be able to offer general guidance or insights based on the information you provide.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--4f1d2579-cb98-45d9-a06c-3c06aa12e568-0' usage_metadata={'input_tokens': 11, 'output_tokens': 730, 'total_tokens': 741, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 663}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Getting Started with Langchain with Google Gemini\""
      ],
      "metadata": {
        "id": "7bvbt8XMrHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Force LangSmith tracing\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Google Gemini\"\n",
        "\n",
        "# Create LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# This should now create a trace\n",
        "result = llm.invoke(\"Hello, can you confirm tracing works?\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nICX2g2Xsey0",
        "outputId": "d2689167-ef47-4747-eaff-a01999dcb706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=e3748894-e94d-42f9-b7ce-a45c8111e8bc,id=e3748894-e94d-42f9-b7ce-a45c8111e8bc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='That\\'s an interesting question!\\n\\nIn the context of an AI like me, \"tracing\" doesn\\'t work in the traditional software debugging sense (i.e., stepping through lines of code). However, there are analogous concepts:\\n\\n1.  **Internal Thought Process / Chain of Thought:** When I process a complex query, I often go through an internal \"thought process\" or \"scratchpad\" where I break down the problem, consider different angles, retrieve information, and formulate my response. This is sometimes referred to as \"chain of thought\" reasoning. While you don\\'t see this raw internal trace directly in my final output (unless I\\'m explicitly prompted to \"think step-by-step\"), it\\'s how I arrive at my answers.\\n\\n2.  **Prompt Engineering for Transparency:** You can, in a sense, *induce* me to \"trace\" my reasoning for you. If you ask me to \"explain my reasoning,\" \"show my work,\" or \"break it down step-by-step,\" I can output the intermediate steps of my thought process, making my \"trace\" visible to you.\\n\\n3.  **Development and Debugging (for the creators):** For the engineers and researchers who built and maintain me, there are indeed extensive logging, monitoring, and debugging tools that allow them to \"trace\" my internal operations, performance, and identify areas for improvement or error.\\n\\nSo, while I don\\'t have a literal \"trace\" function like a traditional program debugger, my internal mechanisms and the ability to articulate my reasoning provide a similar kind of transparency.\\n\\n**In short: Yes, in a conceptual way, my internal processes involve a form of \"tracing\" to arrive at answers, and I can often be prompted to show that \"trace\" to you.**' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--e3748894-e94d-42f9-b7ce-a45c8111e8bc-0' usage_metadata={'input_tokens': 9, 'output_tokens': 1327, 'total_tokens': 1336, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 956}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "jReKJh4NZzKP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain=prompt | llm"
      ],
      "metadata": {
        "id": "rNT5pfuKvXnS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=chain.invoke({\"input_language\": \"English\",\n",
        "        \"output_language\": \"Hindi\",\n",
        "        \"input\": \"I love programming.\",})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Wn44GcxYmb",
        "outputId": "dfdf1fb0-9511-44ae-efb6-7292059caa77"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=7fc804ad-b8a5-44aa-bdc4-c73027d38a8b,id=7fc804ad-b8a5-44aa-bdc4-c73027d38a8b; trace=7fc804ad-b8a5-44aa-bdc4-c73027d38a8b,id=d76ce5a7-55e8-4cc0-8a0f-9e8eaf472cc6; trace=7fc804ad-b8a5-44aa-bdc4-c73027d38a8b,id=d76ce5a7-55e8-4cc0-8a0f-9e8eaf472cc6; trace=7fc804ad-b8a5-44aa-bdc4-c73027d38a8b,id=07d25279-6771-4ee4-b7f1-39cef92c027f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3DcWyElxmGW",
        "outputId": "0a10989a-4616-4df5-df08-53f2dc69c263"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='मुझे प्रोग्रामिंग करना बहुत पसंद है। (Mujhe programming karna bahut pasand hai.)' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--07d25279-6771-4ee4-b7f1-39cef92c027f-0' usage_metadata={'input_tokens': 16, 'output_tokens': 481, 'total_tokens': 497, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 463}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "dQMDgmwVx_YC",
        "outputId": "8095e1dd-1f36-44ee-cbe4-e175a06fc1fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 154);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "CVVLopvpyMc6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain= prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "Ce-SpB04zAQ-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=chain.invoke({\"input_language\": \"English\",\n",
        "        \"output_language\": \"Hindi\",\n",
        "        \"input\": \"I love programming.\",})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYZb9zdQzKHz",
        "outputId": "990ef62c-9c06-45b8-eb27-a116c7c56d6a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=e6dcb0ad-411e-4d58-8a2a-11348b9c4937,id=e6dcb0ad-411e-4d58-8a2a-11348b9c4937; trace=e6dcb0ad-411e-4d58-8a2a-11348b9c4937,id=37dcb8f6-713f-48c4-b7c9-87258bf80f6f; trace=e6dcb0ad-411e-4d58-8a2a-11348b9c4937,id=37dcb8f6-713f-48c4-b7c9-87258bf80f6f; trace=e6dcb0ad-411e-4d58-8a2a-11348b9c4937,id=d1424a39-57d5-4d6f-b580-3de3a3e8948e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_AUXiytzOMP",
        "outputId": "8ea4f87f-5ac5-420e-eeac-368e4d97d412"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मुझे प्रोग्रामिंग बहुत पसंद है।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LikmuTywzTRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}